{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'Analyse des correspondances principales \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Introduction et présentation des objectifs du document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ce notebook est dédié à l'analyse en composantes principales (ACP ou PCA en anglais pour principal component analysis). Il doit permettre au lecteur de découvrir les avantages de Python pour la réalisation d'une ACP, de se familiariser avec la syntaxe de ce langage de programmation mais aussi de se pencher sur la théorie mathématique qui donne sens à l'ACP. En effet, il nous semble que pour analyser de la façon la plus juste qui soit les résultats d'une ACP, il est insuffisant de se former à la seule interprétation des données une fois traitées.   \n",
    "Faire l'effort de comprendre la théorie mathématique sur laquelle repose l'ACP peut permettre au chercheur en sciences sociales d'enrichir son interprétation et d'éviter des erreurs. Cependant, bien qu'il soit important que le praticien en sciences sociales ait une bonne maîtrise de son outil d'analyse, aborder la théorie mathématique de l'ACP sous un angle trop formel pourrait nous faire perdre de vue l'usage pratique de l'ACP en sciences sociales. \n",
    "L'implémentation informatique des calculs mathématiques assure selon nous le lien entre théorie et pratique. Elle nous permet de ne pas céder à la tentation de faire une trop grande place à la théorie, et définit les limites de ce tutoriel, guidé par l’objectif pratique de coder un algorithme pour l’ACP. \n",
    "\n",
    "\n",
    "Introduite par les travaux de Karl Pearson en 1901 et mise au point par Hotelling en 1933, cette méthode qui emprunte à la géométrie, à l'algèbre linéaire et à la statistique fait partie de la grande famille des analyses de données multivariées et plus précisément des analyses factorielles.    \n",
    "\n",
    "Son principe est de traiter par le calcul un grand nombre de données quantitatives complexes et parfois redondantes afin de mettre en lumière la structuration de ces données. C'est donc un outil précieux aussi bien en sciences des données qu'en sciences humaines et sociales.   \n",
    "\n",
    "**Concrètement**, face à un tableau de données (ici quantitatives) avec un nombre de colonnes et un nombre de lignes importants (a minima supérieurs à trois), l’ACP permet de compresser et de synthétiser l’information pour dégager des tendances globales qui guideront le chercheur dans son analyse. Les outils graphiques permettent de visualiser les principaux points communs et différences des individus au sein d’une population, comment ces caractéristiques s’assemblent le plus souvent pour former des individus types, ou encore quels individus s’éloignent des schémas plus fréquents. Charge ensuite au chercheur de réfléchir aux pourquoi et aux comment, l’ACP étant uniquement un outil descriptif qui pourra s’ajouter mais jamais se substituer au travail d’analyse. \n",
    "\n",
    "\n",
    "**Techniquement**, réaliser une ACP consiste à construire de nouvelles variables, appelées facteurs, à partir de la combinaison linéaire des variables initiales (en algèbre linéaire, on parle d’opération de changement de base). Suite à ces manipulations, les facteurs obtenus présentent le double intérêt d’être non-corrélés entre eux et de variance (c’est à dire de capacité à représenter les données) décroissantes : les premiers facteurs offrent ainsi la meilleure synthèse de l'information  contenue dans nos données. \n",
    "Le choix d'un nombre limité de facteurs, dits principaux, permet ensuite de réduire le nombre de variables considérées tout en minimisant la perte d’information (on parle de réduction de dimensionalité). Cela nous permettra par exemple de visualiser la structure des données sur un plan 2D (alors que nos données initiales étaient représentées dans un tableau c’est-à-dire, traduit graphiquement, par un nuage de points dans un espace de dimension égale au nombre de variables considérées, souvent dans un espace à bien plus de trois dimensions !).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Le point de départ de ce travail est un double constat :** \n",
    "\n",
    "Il existe une vaste littérature sur le traitement informatisé des données quantitatives. Cependant, il nous a semblé que les travaux existants étaient d'une part majoritairement dédiés non pas à Python mais à R (logiciel libre dédié aux statistiques et qui possède son propre langage également appelé R) et que, d'autre part, ces travaux se présentaient davantage sous la forme de tutoriels pour la réalisation pratique de l'ACP, sans rappel sur la théorie sous-tendue par cette méthode.  \n",
    "\n",
    "L'objectif de ce notebook est donc triple : \n",
    "\n",
    "   - rappeler à l'utilisateur praticien les fondements théoriques de l’ACP (pour qu'il puisse s'en servir efficacement et avec un minimum d’erreurs) tout en conservant une approche empirique, avec pour objectif de concilier la rigueur nécessaire à la compréhension de l'instrument d'analyse sans pour autant rebuter le lecteur par un trop grand formalisme. \n",
    "   - écrire un algorithme d'ACP sur Python qui reprenne les résultats et les visualisations proposées par R ;\n",
    "   - enrichir la gamme et les fonctionnalités de visualisations grâce à la richesse des bibliothèques Python et ainsi donner un premier aperçu de ce qu'un chercheur en sciences sociales pourrait gagner à se familiariser avec la programmation dans ce langage ;  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Bref descriptif des bibliothèques utilisées \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce tutoriel, nous nous servirons de six bibliothèques : pandas, numpy, scipy, matplotlib, seaborn et bokeh.  \n",
    "     \n",
    "   \n",
    " - pandas est la bibliothèque dédiée à la manipulation des données en Python. À ce titre, sa maîtrise est indispensable pour l'analyse de données dans ce langage.\n",
    "   \n",
    "   \n",
    " - numpy structure les données sous formes de ndarray (c'est-à-dire de tableaux multidimensionnels assimilables à des matrices). Cette bibliothèque nous permettra d'effectuer les calculs matriciels nécessaire aux traitements des données.   \n",
    " \n",
    "   \n",
    " - scipy est une bibliothèque dédiée au calcul scientifique sur des objets de type ndarray utilisés par numpy. Nous l'utilisons en particulier pour la décomposition en valeurs singulières   \n",
    " \n",
    "\n",
    " - matplotib est la bibliothèque graphique utilisée en traitement scientifique avec Python. Nous nous en servirons pour restituer nos résultats et rendre leur interprétation plus aisée.\n",
    " \n",
    " \n",
    " - seaborn est basé sur matplotlib mais permet des visualisations plus poussées. Cette bibliothèque est très utilisée pour les représentations graphiques de statistiques.\n",
    "   \n",
    "   \n",
    " - bokeh permet de faire des figures interactives, étendant ainsi la gamme des possibles en visualisation.\n",
    " \n",
    " ---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notre cas d'exemple : un sondage de confiance  \n",
    "\n",
    "Dans ce tutoriel, nous appliquerons notre algorithme d'ACP aux résultats d'un sondage réalisé dans le cadre de l'épidémie de Covid-19.   \n",
    "Un questionnaire de 12 questions a été soumis à 2014 individus. Pour chacun des 12 items (Justice, Média, Police, Assemblée Nationale, Science, Administration, Grandes entreprises, Associations, Gouvernement, Agences gouvernementales liées à la santé et à l'environnement, Médecins,  Sénat), les sondés ont répondu à la question \"Avez-vous confiance ou pas confiance dans… ?\" par un entier naturel entre 0 (pas du tout confiance) et 5 (tout à fait confiance).   \n",
    "En \"langage statistique\", les données obtenues correspondent à un échantillon de 2014 réalisations de 12 variables aléatoires discrètes rassemblées dans un tableau rectangulaire de dimension 2014x12.   \n",
    "Nous nommerons ce tableau df (pour dataframe), et nous l'assimilerons à une matrice 2014x12, avec $ x_i^j$ la valeur prise par la variable n°j pour le i-ième individu ($\\forall i\\in\\{0,...,2013\\}, \\forall j\\in\\{0,...,11\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape est d'importer les bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "import seaborn as sns\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, ColumnDataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "Importons nos données en format en pandas. Nous utiliserons la méthode .head() pour afficher les 5 premières lignes de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Justice</th>\n",
       "      <th>Média</th>\n",
       "      <th>Police</th>\n",
       "      <th>AN</th>\n",
       "      <th>Science</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Entreprises</th>\n",
       "      <th>Associations</th>\n",
       "      <th>Gouvernement</th>\n",
       "      <th>Agences</th>\n",
       "      <th>Médecins</th>\n",
       "      <th>Sénat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1009</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1010</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1011</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1012</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Justice  Média  Police   AN  Science  Administration  \\\n",
       "0              0      4.0    5.0     2.0  2.0      1.0             1.0   \n",
       "1              1      3.0    3.0     3.0  3.0      3.0             3.0   \n",
       "2              2      2.0    2.0     2.0  3.0      3.0             2.0   \n",
       "3              3      4.0    5.0     4.0  5.0      2.0             4.0   \n",
       "4              4      5.0    5.0     1.0  4.0      5.0             4.0   \n",
       "...          ...      ...    ...     ...  ...      ...             ...   \n",
       "1009        1009      4.0    4.0     1.0  4.0      2.0             2.0   \n",
       "1010        1010      4.0    4.0     2.0  4.0      4.0             4.0   \n",
       "1011        1011      2.0    4.0     4.0  4.0      2.0             3.0   \n",
       "1012        1012      2.0    5.0     5.0  4.0      2.0             4.0   \n",
       "1013        1013      1.0    1.0     1.0  4.0      2.0             1.0   \n",
       "\n",
       "      Entreprises  Associations  Gouvernement  Agences  Médecins  Sénat  \n",
       "0             2.0           2.0           2.0      2.0       2.0    2.0  \n",
       "1             3.0           3.0           3.0      3.0       3.0    3.0  \n",
       "2             2.0           1.0           3.0      3.0       1.0    3.0  \n",
       "3             4.0           2.0           5.0      4.0       2.0    4.0  \n",
       "4             4.0           4.0           5.0      5.0       2.0    4.0  \n",
       "...           ...           ...           ...      ...       ...    ...  \n",
       "1009          3.0           2.0           2.0      4.0       2.0    4.0  \n",
       "1010          2.0           2.0           2.0      2.0       4.0    2.0  \n",
       "1011          3.0           2.0           5.0      2.0       2.0    3.0  \n",
       "1012          5.0           2.0           5.0      2.0       2.0    4.0  \n",
       "1013          2.0           2.0           2.0      1.0       1.0    4.0  \n",
       "\n",
       "[1014 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./confiance_acp.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation de pandas permet le traitement de données massives. Par des commandes simples et plutôt instinctives, cette bibliothèque permet d’accéder et de manipuler nos données.   \n",
    "Par exemple, après une première visualisation rapide, nous souhaitons nous débarrasser de la colonne des index \"Unnamed: 0\" issue de notre fichier csv d'origine car nous pouvons remarquer que pandas ré-indexe automatiquement les lignes du tableau fourni.   \n",
    "Nous proposons une méthode parmi d'autres pour régler ce problème :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Justice</th>\n",
       "      <th>Média</th>\n",
       "      <th>Police</th>\n",
       "      <th>AN</th>\n",
       "      <th>Science</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Entreprises</th>\n",
       "      <th>Associations</th>\n",
       "      <th>Gouvernement</th>\n",
       "      <th>Agences</th>\n",
       "      <th>Médecins</th>\n",
       "      <th>Sénat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Justice  Média  Police   AN  Science  Administration  Entreprises  \\\n",
       "0         4.0    5.0     2.0  2.0      1.0             1.0          2.0   \n",
       "1         3.0    3.0     3.0  3.0      3.0             3.0          3.0   \n",
       "2         2.0    2.0     2.0  3.0      3.0             2.0          2.0   \n",
       "3         4.0    5.0     4.0  5.0      2.0             4.0          4.0   \n",
       "4         5.0    5.0     1.0  4.0      5.0             4.0          4.0   \n",
       "...       ...    ...     ...  ...      ...             ...          ...   \n",
       "1009      4.0    4.0     1.0  4.0      2.0             2.0          3.0   \n",
       "1010      4.0    4.0     2.0  4.0      4.0             4.0          2.0   \n",
       "1011      2.0    4.0     4.0  4.0      2.0             3.0          3.0   \n",
       "1012      2.0    5.0     5.0  4.0      2.0             4.0          5.0   \n",
       "1013      1.0    1.0     1.0  4.0      2.0             1.0          2.0   \n",
       "\n",
       "      Associations  Gouvernement  Agences  Médecins  Sénat  \n",
       "0              2.0           2.0      2.0       2.0    2.0  \n",
       "1              3.0           3.0      3.0       3.0    3.0  \n",
       "2              1.0           3.0      3.0       1.0    3.0  \n",
       "3              2.0           5.0      4.0       2.0    4.0  \n",
       "4              4.0           5.0      5.0       2.0    4.0  \n",
       "...            ...           ...      ...       ...    ...  \n",
       "1009           2.0           2.0      4.0       2.0    4.0  \n",
       "1010           2.0           2.0      2.0       4.0    2.0  \n",
       "1011           2.0           5.0      2.0       2.0    3.0  \n",
       "1012           2.0           5.0      2.0       2.0    4.0  \n",
       "1013           2.0           2.0      1.0       1.0    4.0  \n",
       "\n",
       "[1014 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.columns[1:]] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modalités d'affichage de pandas sont en outre d'une grande souplesse et nous pouvons paramétrer la quantité de données affichée après chaque exécution.\n",
    "Comme nous pouvons le constater dans les exemples précédents, les paramètres par défaut nous donnent les 5 premières et les 5 dernières lignes de données.  \n",
    "Selon les besoins de l'analyse, il est par exemple possible de faire apparaître le tableau dans sa totalité en règlant les options d'affichage : pd.set_option('display.max_rows', df.shape[0])    \n",
    "...ou encore d'accéder simplement aux têtes de colonnes :  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...ou à la réponse d'un individu donné pour une des questions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3,'Médecins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses autres fonctionnalités sont ainsi disponibles (accès à une colonne, manipulation des index, création ou suppression de nouvelles colonnes ou lignes, modification des données, gestion des valeurs manquantes, etc.). \n",
    "Il vous est donc fortement recommandé de vous référer à la documentation pandas pour découvrir les potentiels offerts par cette bibliothèque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse \n",
    "\n",
    "Quelle relation y a-t'il entre nos différentes colonnes et lignes de données ? Une approche pourrait être de calculer le tableau de corrélation entre chaque colonne. Dans le cadre de ce tutoriel, nous allons plutôt recourir à l'Analyse en Composantes Principales, ce qui facilitera notre compréhension de la relation entre les individus et variables.\n",
    "   \n",
    "---\n",
    "\n",
    "Les analyses de données dites \"à la française\" consistent en une série d'opération d'algèbre matriciel. L'objectif de ces procédés est de représenter les données multidimensionnelles initiales dans un espace de dimension inférieure pour faciliter leur interprétation, tout en minimisant la perte d'information liée à cette réduction de dimensionalité.\n",
    "   \n",
    "Or qui dit calcul matriciel, dit utilisation de ndarray numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformons notre dataframe pandas en ndarray (matrice) numpy de floats et calculons les dimensions de cette matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy().astype(float)\n",
    "n, p = X.shape\n",
    "print(\"La matrice X est :\\n\\nX ={},\\n\\nElle est de dimension nxp avec n={} et p={}\".format(X,n,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En trois séries étapes, nous allons maintenant réaliser l'ACP tant attendue.**   \n",
    "**#1**\n",
    "Il nous faudra tout d'abord réaliser quelques transformations préliminaires :\n",
    "- attribuer un poids à chaque individu \n",
    "- calculer le centre de gravité associé au nuage des individus (en p=12 dimensions)\n",
    "- centrer les données \n",
    "- calculer la matrice de variance-covariance  \n",
    "- réduire les données à l'aide de la matrice des inverses des écarts-types\n",
    "\n",
    "\n",
    "Ces étapes constituent une *standardisation* des données (le mot anglais *standardization* est cependant plus couramment employé) et font partie d'un prétraitement (ou preprocessing), au même titre que le nettoyage des données, leur agrégation ou leur réajustement potentiels en cas de biais ou de redondance. \n",
    "Par ces opérations, la qualité des données initiales peut être renforcée.\n",
    "\n",
    "Centrer et réduire les données (les \"standardiser\") est une étape très fréquente (voire quasi-systématique) en statistiques. Elle est bien souvent essentielle aussi bien pour le traitement de données en sciences sociales que dans le cadre des *data sciences*. Certaines bibliothèques disposent par conséquent d'une fonction qui automatise ce prétraitement. \n",
    "Le package *preprocessing* de *sklearn* propose ainsi une fonction *StandardScaler* qui permet, en une commande, de centrer et réduire les données.\n",
    "\n",
    "Pourquoi ne pas avoir l'utilisée ?\n",
    "Deux raisons principales expliquent notre choix.  \n",
    "Tout d'abord, comme rappelé en introduction, nous souhaitons que le lecteur ait autant que possible la main sur les opérations (ce qui implique de décomposer les fonctions \"toutes faites\") afin qu'il soit en mesure de mieux comprendre ou de se remémorer les étapes du traitement informatique. \n",
    "Ensuite, parce que la bibliothèque *sklearn* a été conçue pour l'apprentissage automatique, et qu'elle peut, à ce titre, paraître inadaptée pour un chercheur en sciences sociales qui souhaiterait analyser des données préalablement collectées mais en aucun cas réaliser des prédictions grâce au machine learning. \n",
    "\n",
    "**#2**\n",
    "La deuxième série d'étape constitue le coeur de l'ACP. Nous devrons :\n",
    "\n",
    "- effectuer une décomposition en valeurs singulières\n",
    "- choisir le nombre de composantes (ou axes) à retenir au vu des valeurs singulières obtenues et du pourcentage de variance expliquée par chacune\n",
    "- projeter les données sur les nouveaux axes \n",
    "\n",
    "\n",
    "Nous aurons ainsi l'occasion de nous familiariser, voire de comprendre, les calculs mathématiques qui soutiennent l'ACP.  \n",
    "\n",
    "**#3** \n",
    "Enfin, la troisième série d'étapes nous permettra de visualiser nos résultats.   \n",
    "Nous afficherons :\n",
    "- le nuage de point résultant de l'ACP\n",
    "- le cercle de corrélation \n",
    "- les cos2\n",
    "\n",
    "À l'issue de ces derniers affichages, nous aurons rempli le cahier des charges fixé en introduction : nous aurons à notre disposition un outil pratique d'analyse des données et nous aurons pu démontrer les potentialités offertes par Python, en présentant au lecteur-chercheur un nuage de point interactif, indisponible sur R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# #1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la matrice des poids   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est parfois utile de pouvoir modifier le poids attribué à chaque individu (par exemple lorsque les données sont regroupées, qu'il faut les redresser ou encore lorsque l'on souhaite calculer des corrélations sans certains individus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Pour ces données, on accorde la même importance à tous les individus. Tous nos individus \"comptent pour un\".  \n",
    "Le poids $p_{i}$ associé à chaque individu $i\\in\\{1,...,n\\}$ est inversement proportionnel à la taille de l'échantillon tel que $p_{i} = 1/n, \\forall i\\in\\{1,...,n\\}$\n",
    "\n",
    "  \n",
    "Ainsi, dans le cas usuel, la matrice D des poids est donnée par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = (1/n)*np.eye(n) # la fonction np.eye(n) permet de créer une matrice identité de dimension n.   \n",
    "D[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons une matrice diagonale de taille n et de coefficients 1/1014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du centre de gravité du nuage\n",
    "\n",
    "Chacun individu est un point défini par p coordonnées (12 dans notre exemple). L'individu $i\\in\\{1,...,n\\}$ est entièrement décrit par $x_i = (x_i^1,...,x_i^p)$. Autrement dit c'est un point dans un espace à p dimensions appelé l'espace des individus.   \n",
    "Dans cet espace, repèré par une origine et des axes orthonormés, l'ensemble des n individus forme un nuage de points. \n",
    "On peut calculer le centre de gravité g de ce nuage. \n",
    "g est lui-même un point de l'espace des individus. Chacune des $g^{j\\in\\{1,...,p\\}}$ coordonnées est la moyenne arithmétique des coordonnées $x^j $ des individus.\n",
    "g = ($\\bar{x^1},\\bar{x^2},..., \\bar{x^p}$)\n",
    "\n",
    "g est obtenu par combinaison linéaire des $x_i$. La fonction de $\\mathbb{R}^p$ à valeur dans $\\mathbb{R}$, qui aux $x_i$ attribue g est donc une application linéaire. À ce titre, elle peut être associée à une matrice et le calcul de g à un calcul matriciel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = X.T.dot(D).dot(np.ones(n))  # np.ones((n,p)) crée une matrice de dimension n.p ne contenant que des 1 (par défaut np.ones(n)= np.ones((n,)))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, toutes les opérations que nous réaliserons sur nos données répondent à certaines propriétés de commutativité et de stabilité par produit et par somme : nous souhaitons observer nos données sous un autre angle, qui facilite leur interprétation, mais les transformations appliquées doivent être réversibles et ne pas modifier la structure de nos données. Ainsi tous les calculs pourront être réalisés sous forme matricielle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par ailleurs, j'attire votre attention sur le fait qu'avoir décomposé la standardisation en ses étapes successives peut dès à présent nous être utile.   \n",
    "En effet, le centre de gravité $g$ est, en lui-même, un premier indicateur sur nos données. Il renseigne sur la tendance centrale de notre échantillon.  \n",
    "Mettons en forme notre exemple :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = pd.DataFrame(g).transpose()\n",
    "g_df.columns = list(df)\n",
    "g_df.index = ['g']\n",
    "g_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si les 12 variables aléatoires considérées suivaient un loi discrète uniforme, pour un nombre suffisant de sondés, $g$ serait très proche du vecteur ligne de coefficients tous égaux à (1+2+3+4+5)/5 = 3.\n",
    "Or la valeur de $g$ est significativement inférieure à 3 pour les colonnes \"Science\" et \"Médecins\" par exemple. Cela semble indiquer une plus grande défiance envers ces institutions ou ces corps de métiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau centré associé à X\n",
    "\n",
    "En centrant nos données, nous déplaçons simplement l'origine du repère au niveau du centre de gravité de notre nuage de données.   \n",
    "\n",
    "\n",
    "Le tableau des données centrées est la matrice Y tel que $y_i^j = x_i^j - \\bar{x^j}$   \n",
    "Ce qui est équivalent à $y_i^j = x_i^j - g^j$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X - g.T \n",
    "Y_df = pd.DataFrame(Y)\n",
    "Y_df.columns = list(df)\n",
    "Y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir retranché g à chaque $x_i$, la moyenne de l'échantillon est nulle.  \n",
    "Vraiment ? En réalité, (et depuis le début !) numpy ne considère que 8 chiffres significatifs (ce paramètre peut être modifié pour obtenir une plus grande précision).  \n",
    "Calculons le nouveau centre de gravité $g_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.T.dot(D).dot(np.ones(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, en effectuant par exemple la moyenne des $y_1$ pour obtenir la première coordonnée de $g_y$ nous obtenons $0$ à $10^{-14}$ près.\n",
    "Si vous n'êtes pas convaincus par le calcul matriciel, voyez plutôt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_y = []\n",
    "for i in range (12): \n",
    "    g_y.append(np.mean(Y[:,i]))\n",
    "    \n",
    "g_y_df = pd.DataFrame(g_y)\n",
    "g_y_df.index = list(df)\n",
    "g_y_df.columns = ['g_y']\n",
    "g_y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la matrice de variance-covariance \n",
    "\n",
    "La **variance** est un indicateur statistique essentiel qui peut, bien plus qu'une simple moyenne, renseigner sur la **structure** des données.   \n",
    "Une confiance moyenne de 3/5 peut en effet résulter d'une concentration des réponses autour de l'espérance de la loi uniforme discrète *U*([1,n]), ou au contraire d'une très forte polarisation des réponses.   \n",
    "Si, dans notre exemple, tous les sondés ont répondu qu'ils avaient *moyennement* confiance dans l'administration par exemple, on ne peut pas en déduire les mêmes analyses que si la moitié du groupe sondé a exprimé n'avoir pas du tout confiance (score = 1) en cette institution, tandis que l'autre moitié se dit totalement en confiance (score = 5).   \n",
    "\n",
    "La variance indique ainsi la **dispersion** des données autour de la moyenne : plus elle est faible, plus les données sont concentrées autour du point moyen; à l'inverse une forte polarisation des données sera retranscrit par une variance élevée. \n",
    "\n",
    "\n",
    "La **covariance** se distingue de la variance car elle prend en compte les variables deux à deux et nous renseigne sur leur corrélation. \n",
    "Deux variables $X_1$ et $X_2$ auront une covariance positive si les écarts entre $X_1$ et $X_2$ et leurs moyennes respectives ont tendance à être de même signe. Leur covariance sera négative si, au contraire, les écarts ont tendance à être de signes opposés.   \n",
    "\n",
    "Dans notre exemple, si les personnes confiantes dans la police ont tendance à avoir également confiance dans la justice, alors les variables $Police$ et $Justice$ seront corrélées positivement. \n",
    "\n",
    "On pourra ainsi identifier des affinités électives entre les variables ou encore des profils-types d'individus.\n",
    "\n",
    "La matrice de variance-covariance permet de généraliser ces deux notions. Les variances (qui ne sont autres que les covariances des variables avec elles-mêmes) apparaissent sur la diagonale de cette matrice carrée tandis qu'on retrouve la covariance des variables $X_i$ et $X_j$ à l'intersection de la ligne i avec la colonne j.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ainsi, la matrice de variance-covariance des p variables est telle que   \n",
    "\\begin{equation*}\n",
    "V_{p,p} = \n",
    "\\begin{pmatrix}\n",
    "s_1^2 & s_{1,2} & \\cdots & s_{1,p} \\\\\n",
    "s_{2,1} & s_2^2 & \\cdots & s_{2,p} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "s_{p,1} & s_{p,2} & \\cdots & s_p^2 \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Avec\n",
    "$\\begin{align}\n",
    "s_{k,l} & = \\frac{1}{n} \\cdot {\\sum_{i=1}^{n} (x_i^k \\cdot x_i^l}- \\bar{x^k} \\cdot \\bar{x^l}), \\ \\ \\forall k,l\\in\\{1,...,p\\}\n",
    "\\end{align}$\n",
    "\n",
    "D'où sous forme matricielle :   \n",
    "\n",
    "\\begin{matrix} \n",
    "V = X^T DX - gg^T \n",
    "\\end{matrix}  \n",
    "En effet : \n",
    "\\begin{equation*}\n",
    "X^T \\cdot X = \n",
    "\\begin{pmatrix}\n",
    "    x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\\n",
    "    x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "    x_1^p & x_2^p & \\cdots & x_n^p \n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "    x_1^1 & x_1^2 & \\cdots & x_1^p \\\\\n",
    "    x_2^1 & x_2^2 & \\cdots & x_2^p \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "    x_n^1 & x_n^2 & \\cdots & x_n^p \n",
    "\\end{pmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "X^T \\cdot X = \n",
    "\\begin{pmatrix}\n",
    "    \\sum_{i=1}^{n} x_i^1 \\cdot x_i^1 & \\sum_{i=1}^{n} x_i^1 \\cdot x_i^2 & \\cdots & \\sum_{i=1}^{n} x_i^1 \\cdot x_i^p \\\\\n",
    "    \\sum_{i=1}^{n} x_i^2 \\cdot x_i^1 & \\sum_{i=1}^{n} x_i^2 \\cdot x_i^2 & \\cdots & \\sum_{i=1}^{n} x_i^2 \\cdot x_i^p \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "    \\sum_{i=1}^{n} x_i^p \\cdot x_i^1 & \\sum_{i=1}^{n} x_i^p \\cdot x_i^2 & \\cdots & \\sum_{i=1}^{n} x_i^p \\cdot x_i^p \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par ailleurs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{n} y_i^k \\cdot y^l_i & = \\sum_{i=1}^{n} (x_i^k - \\bar{x^k})(x_i^l - \\bar{x^l}) \\\\\n",
    "& = \\sum_{i=1}^{n} (x_i^k \\cdot x_i^l - x_i^k \\cdot \\bar{x^l} - \\bar{x^k} \\cdot x_i^l + \\bar{x^k} \\cdot \\bar{x^l})\\  par\\ développement\\ du\\ produit \\\\\n",
    "& = \\sum_{i=1}^{n} (x_i^k \\cdot x_i^l) - \\bar{x^l} \\cdot (\\sum_{i=1}^{n} x_i^k) - \\bar{x^k} \\cdot (\\sum_{i=1}^{n} x_i^l) + n (\\bar{x^k} \\cdot \\bar{x^l}) \\ par\\ linéarité\\ de\\ la\\ somme \\\\\n",
    "& = \\sum_{i=1}^{n} (x_i^k \\cdot x_i^l) - \\bar{x^l} \\cdot (n \\bar{x^k}) - \\bar{x^k} \\cdot (n \\bar{x^l}) + n (\\bar{x^k} \\cdot \\bar{x^l}) \\ car\\ \\bar{x^k} = 1/n \\cdot(\\sum_{i=1}^{n} x_i^k), \\forall k\\in\\{1,...,p\\}\\\\\n",
    "& = \\cdot {\\sum_{i=1}^{n} x_i^k \\cdot x_i^l}- n (\\bar{x^k} \\cdot \\bar{x^l}) \\ par\\ simplification \\\\\n",
    "& = n \\cdot s_{k,l}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi : \n",
    "\\begin{equation*}\n",
    "Y^T \\cdot Y = n \\cdot V\n",
    "\\end{equation*}\n",
    "\n",
    "Et en multipliant à gauche et à droite par D, on obtient la matrice de variance-covariance V :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Y.T.dot(D).dot(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoir une approche mathématique du traitement des données nous permet de vérifier les calculs à tout moment et sans gros effort. \n",
    "On peut par exemple vérifier la valeur de la variance $s_1^2 = 1/n \\cdot {\\sum_{i=1}^{n} ((x_i^1)^2}- (\\bar{x^1})^2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_1 = 0\n",
    "for x in range(n):\n",
    "    s1_1 += X[x,0]*X[x,0]- g[0]**2\n",
    "s1_1 *=1/n\n",
    "\n",
    "round(s1_1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est effectivement la covariance de la première variable avec elle-même :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V[:1, :1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mettant en forme variances et covariances, nous pouvons faire de nouvelles interprétations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_df = pd.DataFrame(V)\n",
    "V_df.columns = list(df)\n",
    "V_df.index =list(df)\n",
    "V_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(V == np.min(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note donc, par exemple, que $Police$ et $Associations$ sont les variables avec la plus petite covariance. \n",
    "La défiance envers la police va donc relativement de paire avec la confiance envers les associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(V == np.max(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre exemple : la confiance accordée au gouvernement est très polarisée puisque la variance de cette variable est élevée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau des données centrées et réduites\n",
    "\n",
    "La matrice des variances-covariances peut cependant s'avérer trompeuse : si les valeurs prises par les variables ne sont pas exprimées sur la même échelle, on risque fortement de comparer des choux avec des carottes et d'en déduire de fausses interprétations.   \n",
    "\n",
    "Ainsi il est fort probable que si l'on modifiait une question du sondage et que l'on demandait désormais aux sondés d'exprimer leur confiance envers les entreprises sur une échelle de 1 à 100, tandis que les autres institutions sont toujours notées sur une échelle de 1 à 5, la variance associée à la variable $Enteprises$ deviendrait supérieure à celle associée à $Gouvernement$.  \n",
    "Doit-on en déduire que les avis concernant les entreprises ont changés ? Qu'ils se sont davantage polarisés ? Non, bien sûr, il s'agit simplement d'un souci d'échelle qu'il faut pouvoir corriger.   \n",
    "\n",
    "C'est tout l'intérêt de la réduction, obtenue en divisant les valeurs associées à chaque variable par l'écart type de cette dernière (c'est-à-dire en multipliant la matrice centrée par la matrice des inverses des écarts-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_invS = np.eye(p)\n",
    "#D_invS2 = np.eye(p)\n",
    "#D_invS3 = np.eye(p)\n",
    "\n",
    "for x in range(p):\n",
    "#    D_invS2[x,x]= 1/V[x,x]\n",
    "    D_invS[x,x]= 1/np.sqrt(V[x,x])\n",
    "\n",
    "\n",
    "#D_invS3[x,x] = np.sqrt(D_invS2[x,x])\n",
    "#D_invS3\n",
    "\n",
    "Z = Y.dot(D_invS)\n",
    "\n",
    "Z_df = pd.DataFrame(Z)\n",
    "Z_df.columns = list(df)\n",
    "\n",
    "Z_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre exemple, puisque toutes nos valeurs étaient exprimées sur la même échelle dans la même unité, nous pouvons continuer les calculs avec Z (la matrice centrée-réduite) ou Y (la matrice centrée non-réduite) indifféremment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "L'étape de pré-traitement est désormais terminée, attaquons nous sans plus attendre à l'ACP à proprement parlé. \n",
    "\n",
    "---\n",
    "# #2 Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décomposition en valeurs singulières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe de l'ACP est de déterminer une base orthonormée telle que l'inertie (autrement dit la variance) du nuage projeté y soit maximale.  \n",
    "\n",
    "\n",
    "En particulier, choisir le premier axe revient à trouver un vecteur $u$ tel que la projection de Z sur $u$ ait une variance maximale. Autrement dit, ce vecteur, aussi appelé facteur principal, maximise $Var(Z\\cdot u) = (Z\\cdot u)^T \\cdot D \\cdot (Z\\cdot u)$ selon la formule de la variance vu précédemment.\n",
    "\n",
    "D'où :   \n",
    "$\\begin{align}\n",
    "Var(Z\\cdot u) & = u^T \\cdot Z^T \\cdot D \\cdot Z\\cdot u \\\\\n",
    "& = u^T \\cdot (Z^T \\cdot D \\cdot Z)\\cdot u \\\\\n",
    "& = u^T \\cdot V \\cdot u\n",
    "\\end{align}$   \n",
    "avec V la matrice de variances-covariances.   \n",
    "\n",
    "Pour obtenir une base orthornormée, il nous faut diagonaliser la matrice V (car V est une matrice symétrique et que toute matrice réelle symétrique est diagonalisable par une matrice orthogonale).\n",
    "La formule du changement de base pour les formes bilinéaires nous donne : \n",
    "$V = P^T\\cdot V' \\cdot P$ avec P la matrice de passage de l'espace d'arrivée à l'espace de départ.   \n",
    "Ainsi, en remplaçant V dans l'équation ci-dessus :   \n",
    "$\\begin{align}\n",
    "Var(Z\\cdot u)&= v^T \\cdot (P^T\\cdot V' \\cdot P) \\cdot u \\\\\n",
    "&= (u \\cdot P)^T \\cdot V' \\cdot (P \\cdot u)\n",
    "\\end{align}$   \n",
    "\n",
    "V' est la matrice diagonale des valeurs propres $\\lambda_i, \\forall i\\in\\{1,...,12\\}$ associées à V.    \n",
    "On a ainsi que le vecteur $u$ qui maximise $Var(Z\\cdot u)$ est tel que $(u \\cdot P)^T$ est le vecteur propre associé à la plus grande valeur propre de V. En ordonnant les valeurs propres par ordre décroissant, le premier facteur $u_1$ est tel que $(u_1 \\cdot P)^T$ est le vecteur propre associé à $\\lambda_1$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons le même résulat par une décomposition en valeurs singulières de Z.  \n",
    "En effet, si Z avait été une matrice carrée, nous aurions pu trouver directement les valeurs propres de Z et ses vecteurs propres associés. \n",
    "Malheureusement, l' échantillon que l'on désire analyser comporte moins de variables que d'individus (ce qui est très souvent le cas) et la notion de valeur propre n'a donc pas de sens pour la matrice rectangulaire Z. \n",
    "\n",
    "Néanmoins, comme nous l'avons remarqué, les produits à gauche et à droite de Z par sa transposée (c'est-à-dire la variance) sont bien des matrices carrées qui sont en outre symétriques semi définies positives. Ces produits admettent donc des valeurs propres (cf ci-dessus). \n",
    "\n",
    "On appellera donc valeurs singulières de Z, noté $\\sigma$ en mathématiques et $s$ dans notre code, les racines carrées des valeurs propres de V.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour peu que nous sachions diagonaliser une matrice (V, en l'occurrence), nous disposons désormais de tous les outils pour réaliser les calculs matriciels nécessaires à l'obtention des valeurs singulières de Z et à ses facteurs principaux.  \n",
    "\n",
    "Le calcul des valeurs singulières et des vecteurs associés a été optimisé par le package linalg de la bibliothèque scipy qui comprend une fonction de décomposition en valeurs singulières (ou SVD en anglais). Nous utiliserons donc cette fonction mais nous encourageons le lecteur a faire manuellement les calculs matriciels qui le mèneront aux mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s,Vh = linalg.svd(Z, full_matrices=False, # It's not necessary to compute the full matrix of U or V\n",
    "                     compute_uv=True)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- s correspond aux p=12 valeurs singulières   \n",
    "- Vh[:k] nous donnent les principaux composants    \n",
    "- la formule $U[:, :k] \\cdot s[:k]$ nous permet de projeter les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La partie la plus aride du traitement est déjà terminée !\n",
    "\n",
    "Reste à déterminer le nombre de facteurs que l'on souhaite retenir pour notre ACP.  \n",
    "Ce choix est très important pour l'analyse : il fixe la limite entre l'information que l'on peut négliger (car on peut l'attribuer à des fluctuations aléatoires) et celle qu'il faut prendre en compte. Pour reprendre l'analogie faite par le professeur Ricco Rakotomalala dans ses notes de cours, il faut conserver le *signal* et ignorer le *bruit*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs critères et procédures peuvent ainsi nous aider dans le choix du nombre de facteurs propres à retenir.\n",
    "Nous avons choisi d'utiliser la méthode très visuelle de l'*éboulis des valeurs propres*, qui n'est autre que la représentation graphique de la valeur prise par les différentes valeurs propres (toujours ordonnées par ordre décroissant).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Éboulis des valeurs propres\n",
    "\n",
    "Déterminer le nombre de facteurs à retenir pour notre analyse revient alors à rechercher, s’il existe, le “coude” dans le graphe des valeurs propres de V ou dans celui des valeurs singulières $s$ de Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ebl = plt.subplots(figsize = (10,7))\n",
    "\n",
    "ebl.plot(range(p), s, marker= 'x', c='k')\n",
    "\n",
    "ebl.bar(range(p),s[:])\n",
    "\n",
    "\n",
    "for i,j in zip(range(p),np.around(s, decimals=3)):\n",
    "    ebl.annotate(str(j),xy=(i+0.1,j+0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ebl.set_ylabel('valeur propre')\n",
    "ebl.set_xlabel('dimensions')\n",
    "ebl.set_title('Éblouis des valeurs propres')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La consigne est ensuite de ne conserver que les vecteurs associés aux valeurs propres (ou singulières) placées avant le coude. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Règle de Kaiser-Guttman \n",
    "\n",
    "Une autre méthode peut-être obtenue avec les valeurs propres calculées à partir des valeurs singulières :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = (s**2)/(n-1)\n",
    "lambdas_df = pd.DataFrame(lambdas).transpose()\n",
    "lambdas_df.index = ['valeurs propres']\n",
    "lambdas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque l'ACP est normée, la somme des valeurs propres est égale au nombre de variables (aux erreurs d'arrondis près) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et leur moyenne vaut 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus si les variables étaient deux à deux orthogonales (et donc les facteurs de corrélations identiques pour chaque variables deux à deux), les valeurs propres issues de l’analyse seraient toutes égales à 1 i.e. tous les facteurs seraient aussi importants les uns que les autres, chaque valeur propre exprimant $1/p =1/12$ de l'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque ce n'est pas le cas, il paraît pertinent de ne considérer que les facteurs (ou axes) dont la valeur propre associée est supérieure à 1 ce qui, dans notre exemple, correspond aux deux premiers facteurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau des valeurs propres et pourcentage de variance expliquée par chacune  \n",
    "\n",
    "Enfin, on peut considérer une troisième méthode, souvent associée de façon complémentaire au graphe de l'éboulis mais qui nécessite le calcul des valeurs propres (alors que les valeurs singulières pouvaient être suffisantes pour l'éboulis). \n",
    "On s'intéresse ici aux pourcentages d'inertie expliquée par les axes associés aux valeurs propres. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variance_percent = lambdas *100/p\n",
    "\n",
    "cumulative_variance_percent = [ sum(variance_percent[:i]) for i in range(p+1)  ][1:]\n",
    "\n",
    "tab_inertie = pd.DataFrame(np.array([lambdas, variance_percent, cumulative_variance_percent]), index=['valeurs propres',  'pourcentage variance', 'variance cumulées (en %)']).transpose()\n",
    "tab_inertie = tab_inertie.round({'variance cumulées (en %)': 0, 'pourcentage variance' :0, 'valeurs propres':3})\n",
    "tab_inertie.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, on peut se rassurer en observant que nous n'avons pas perdu d'information : conserver les 12 axes, nous donne une variance cumulée de 100%. Nous n'avons donc pas fait n'importe quoi avec nos données !\n",
    "En revanche, on remarque également que, en choisissant de ne conserver que deux axes, nous acceptons de perdre 45% de l'information... cela représente une perte importante pour l'analyse et cette dernière observation pourrait nous conduire à augmenter le nombre de facteurs retenus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, l'analyse faite sur les 2 premiers facteurs n'est souvent qu'un début, et elle doit fréquemment être complétée par la prise en compte du troisième, du quatrième et jusqu'à un n-ième facteur (ici vraisemblablement jusqu'au cinquième ou sixième facteur, point d'inflexion de la courbe des variances cumulées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3 Visualisations\n",
    "\n",
    "Pour faciliter l'interprétation, pour restituer des résultats, pour communiquer avec le reste de la communauté scientifique ou avec un public plus large... représenter ces données et leur structure est une étape essentielle de l'analyse. L'importance des visualisations dans le processus de production de savoir (ou *d'objets techno-scientifiques* dans le vocabulaire Latourien) est soulignée par de nombreux auteurs (aussi bien par l'anthropolgue Bruno Latour, donc, que par des chercheurs plus spécialisés dans le secteur de la science des données commme Edward Tufte par exemple).   \n",
    "\n",
    "Les tableaux des valeurs propres et de leur pourcentage de variance expliquée, par exemple, peuvent déjà être considérés comme de premières visualisations. Bien souvent, cependant, elles sont rangées parmi les représentations intermédiaires et sont complétées par des graphiques plus facilement appréhendables et interprétables.   \n",
    "\n",
    "\n",
    "\n",
    "Nous ne programmerons ici que quelques unes des visualisations existantes pour l'ACP (les plus fréquemment mobilisées et celles qui sont disponibles sur R). Néanmoins, le lecteur pourra garder en tête que ses capacités de programmation sont la seule limite au champ des visualisations possibles (ce qui ne signifie certes pas pour autant que toute représentation graphique est pertinente). Libre à nous, donc, de faire preuve de créativité pour faire parler nos analyses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualité de représentation des variables\n",
    "### Cercle de corrélation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par projeter nos variables sur nos axes principaux pour déterminer la qualité de notre représentation.\n",
    "Puisque notre ACP est normée (nous avons centré et réduit nos variables), la distance des variables projetées à l’origine g est égale à 1 dans l'espace de dimension p.   \n",
    "Par conséquent, les variables projetées seront dans le cercle de centre 0 (les variables ont été centrées sur le 0 de $R^p$) et de rayon 1. On appelle ce cercle le *cercle des corrélations*.   \n",
    "\n",
    "Les variables sont d'autant mieux représentées par le plan de projection que leur image sur le cercle de corrélation est proche du bord du cercle. En effet, si le premier axe correspondait à la variable $Science$ par exemple, la norme de cette dernière serait de 1, comme dans l'espace en p dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corvar = np.zeros((p,p))\n",
    "for k in range(p):\n",
    "    corvar[:,k] = Vh[k,:] * np.sqrt(lambdas[k]) #corrélations variable-facteur\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15,15)) \n",
    "axes.set_xlim(-1,1) \n",
    "axes.set_ylim(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(p): \n",
    "    if df.columns[j] != \"Sénat\" and df.columns[j]!=\"AN\": #nous avons construit cette boucle car nos légendes se superposaient, il s'agit donc d'un détail de présentation\n",
    "        plt.annotate(df.columns[j],(corvar[j,0]- 0.19,corvar[j,1]-0.01))\n",
    "    else :\n",
    "        plt.annotate(df.columns[j],(corvar[j,0]- 0.1,corvar[j,1]-0.03)) \n",
    "    axes.arrow(0,\n",
    "             0,  \n",
    "             corvar[j,0],\n",
    "             corvar[j,1],\n",
    "             length_includes_head=True,\n",
    "             head_width=0.03,\n",
    "             head_length=1.5 * 0.03)\n",
    "\n",
    "\n",
    "plt.axvline(color='silver',linestyle='--',linewidth=1)\n",
    "plt.axhline(color='silver',linestyle='--',linewidth=1)\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False) \n",
    "axes.add_artist(cercle)\n",
    "\n",
    "axes.set_ylabel('Dim2 ({}%)'.format(np.around(variance_percent[1], decimals=2)))\n",
    "axes.set_xlabel('Dim1 ({}%)'.format(np.around(variance_percent[0], decimals=2)))\n",
    "axes.set_title('Cercle de corrélations des variables')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La qualité de représentation de chaque variable, étant donné nos nouveaux axes, peut également être appréciée par le calcul du carré des coordonnées des variables sur notre nouveau repère. On parle aussi du calcul des $cos^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos2var = corvar**2 \n",
    "labels_cos= ['Cos'+'\\u00B2'+' pour le facteur n°{}'.format(i+1) for i in range(p)]\n",
    "\n",
    "\n",
    "COS2 = pd.DataFrame(cos2var)\n",
    "COS2.index = df.columns\n",
    "COS2.columns = labels_cos\n",
    "COS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette représentation sous forme de tableau peut, je vous l'accorde, ne pas être très parlante. Plusieurs visualisations ont été crée pour faciliter l'interprétation des $cos^2$. Parmi elles, on trouve la *heat map*,  qui fera correspondre la qualité de représentation d'une variable (donnée par son $cos^2$) à un nuancier de couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(280, 220,sep = 1, as_cmap=True)\n",
    "\n",
    "\n",
    "sns.heatmap(COS2, annot=True, xticklabels = ['Dim %d' %i for i in range (1, p+1)], cmap=cmap, vmin=0, vmax=.5, center=0,\n",
    "            square=True, linewidths=5.75, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "ax.set_title('Qualité de représentation des variables sous forme de heat map')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le premier facteur principal représente bien mieux la variable $Assemblée\\ Nationale$, par exemple, que la variable $Association$, qui est relativement mal représentée par les deux premiers facteurs que nous avons conservés. \n",
    "Il faudra donc faire d'autant plus attention de ne pas sur-interpréter les résultats obtenus pour cette variable car nous avons perdu plus de 70\\% de l'information la concernant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Intéressons nous désormais au plan principal, c'est-à-dire à la projection de nos 1014 sondés sur les deux premiers facteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphique des individus \n",
    "\n",
    "Projetons nos données sur les axes principaux à l'aide des résultats obtenus par la décomposition en valeurs singulières :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = np.dot(U, np.diag(s))\n",
    "projected_data = pd.DataFrame(projected_data)\n",
    "projected_data.columns = ['facteur %d' %i for i in range (1, p+1)]\n",
    "\n",
    "projected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré une perte significative d'information, nous avons choisi de ne conserver que les deux premiers facteurs. Dans une analyse approfondie de nos données (ce qui sort du cadre de ce document) le nuage de point ci-dessous  serait à combiner avec le graphique des individus ayant le troisième et le quatrième facteur principal comme axes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "\n",
    "source = ColumnDataSource(\n",
    "         data=dict(\n",
    "            x=projected_data.iloc[:,0],\n",
    "            y=projected_data.iloc[:,1],         \n",
    "         ))\n",
    "\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Numéro de l'individu\", \"$index\"),\n",
    "            (\"Coordonnées de l'individu projeté\", \"($x, $y)\"),  \n",
    "        ], names = ['dots'])\n",
    "\n",
    "fig = figure(plot_width=600, \n",
    "             plot_height=400, \n",
    "             title= 'Nuage des individus',\n",
    "             tools = \"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "fig.add_tools(hover)\n",
    "fig.xaxis.axis_label = 'Dim 1'\n",
    "fig.yaxis.axis_label = 'Dim 2'\n",
    "\n",
    "\n",
    "fig.circle(projected_data.iloc[:,0], projected_data.iloc[:,1], size=3, alpha=0.6, name = 'dots', hover_color='orange')\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetons un oeil aux points les plus extrêmes. Grâce à l'interactivité, nous pouvons obtenir l'index des individus situés le plus à gauche en passant notre souris sur le point correspondant. Il s'agit des individus qui ont répondu de la même façon que l'individu 47. Observons ses réponses :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.loc[47]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'individu 47 se distingue par la grande confiance qu'il accorde à toutes les institutions ! Qu'en est-il du profil situé à l'extrême droite ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.loc[292]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son profil de réponse est caractérisé par une défiance généralisée.  \n",
    "De fait, le premier axe principal, situé en abscisse, distingue les profils *confiants* (ici à gauche) des profils *défiants* (à droite). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'en est-il du second axe ?   \n",
    "Nous avons vu grâce au cercle des corrélations que le deuxième facteur principal opposait un groupe qui a exprimé sa confiance dans l'Assemblée Nationale, le Sénat et le gouvernement, à un autre groupe qui accorde davantage sa confiance aux médecins et à la science.   \n",
    "Cette analyse est elle confirmée par l'observation des individus qui sont sur un même axe vertical mais à l'opposé l'un de l'autre ?   \n",
    "Plaçons nous sur l'axe des ordonnées, à la verticale de l'origine du repère, et observons l'individu situé le plus en hauteur (avec la plus grande ordonnée). Le curseur nous indique que nous observons l'individu 451 dont les réponses au sondage furent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.loc[451]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sénat, Assemblée Nationale et gouvernement se sont vus descerner une note de confiance de 5/5 par ce sondé. En revanche, ce même individu a attribué 1/5 à la Science et aux Médecins.   \n",
    "   \n",
    "Voyons désormais le pendant de cet individu, le point proche de l'axe x=0 avec la plus petite ordonnée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.loc[321]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si le Sénat obtient la note de 4/5, l'Assemblée nationale et le gouvernement sont mal notés tandis que Science et médecins ont les notes respectives 5/5 et 4/5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, grâce à ce nuage projeté des individus et à nos analyses, nous pouvons désormais effectuer des classifications et tester leur robustesse.   \n",
    "Faisons par exemple apparaître deux groupes en fonction de la note de confiance attribuée à la science :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "couleurs =[]\n",
    "for note in df['Science']:\n",
    "    if note > 3:\n",
    "        couleurs.append('red')\n",
    "    else :\n",
    "        couleurs.append('black')\n",
    "\n",
    "\n",
    "\n",
    "source = ColumnDataSource(\n",
    "         data=dict(\n",
    "            x=projected_data.iloc[:,0],\n",
    "            y=projected_data.iloc[:,1],    \n",
    "            groupes_science = couleurs,\n",
    "         ))\n",
    "\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Numéro de l'individu\", \"$index\"),\n",
    "            (\"Coordonnées de l'individu projeté\", \"($x, $y)\"),  \n",
    "        ], names = ['dots'])\n",
    "\n",
    "fig = figure(plot_width=600, \n",
    "             plot_height=400, \n",
    "             title= 'Nuage des individus',\n",
    "             tools = \"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "fig.add_tools(hover)\n",
    "fig.xaxis.axis_label = 'Dim 1'\n",
    "fig.yaxis.axis_label = 'Dim 2'\n",
    "\n",
    "\n",
    "fig.circle('x', 'y', color='groupes_science', source=source, size=3, alpha=1, name = 'dots', hover_color='orange')\n",
    "\n",
    "show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que les notes de confiance supérieures à la moyenne en sciences (en rouge) sont plutôt situées dans le quart bas gauche : la confiance dans la science va souvent de paire avec une moyenne de notes plutôt haute.   \n",
    "\n",
    "\n",
    "Tailles, formes, transparences et couleurs des points peuvent ainsi être modifiées pour mettre en valeur des groupes et vérifier des interprétations :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "couleurs =[]\n",
    "for note in df['Science']:\n",
    "    if note > 3:\n",
    "        couleurs.append('red')\n",
    "    elif note == 3:\n",
    "        couleurs.append('blue')\n",
    "    else :\n",
    "        couleurs.append('black')\n",
    "\n",
    "\n",
    "sizes =[]\n",
    "for note in df['Gouvernement']:\n",
    "    if note < 3:\n",
    "        sizes.append(4)\n",
    "    else :\n",
    "        sizes.append(2)\n",
    "\n",
    "\n",
    "\n",
    "source = ColumnDataSource(\n",
    "         data=dict(\n",
    "            x=projected_data.iloc[:,0],\n",
    "            y=projected_data.iloc[:,1],    \n",
    "            groupes = couleurs,\n",
    "            groupes2 = sizes,\n",
    "         ))\n",
    "\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Numéro de l'individu\", \"$index\"),\n",
    "            (\"Coordonnées de l'individu projeté\", \"($x, $y)\"),  \n",
    "        ], names = ['dots'])\n",
    "\n",
    "fig = figure(plot_width=600, \n",
    "             plot_height=400, \n",
    "             title= 'Nuage des individus',\n",
    "             tools = \"pan, wheel_zoom, box_zoom, reset\")\n",
    "\n",
    "fig.add_tools(hover)\n",
    "fig.xaxis.axis_label = 'Dim 1'\n",
    "fig.yaxis.axis_label = 'Dim 2'\n",
    "\n",
    "\n",
    "fig.circle('x', 'y', color='groupes', source=source, size='groupes2', alpha=1, name = 'dots', hover_color='orange')\n",
    "\n",
    "show(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombreuses sont encore les visualisations que nous pourrions afficher ou/et inventer pour faire parler notre ACP. \n",
    "Néanmoins, notre ambition n'a jamais été l'exhaustivité.   \n",
    "En revanche, nous souhaitions démontrer les potentialités offertes par Python en reproduisant et en complétant certaines fonctionnalités existantes sur R.  \n",
    "La possibilité de créer un nuage des individus interactif, indisponible sur R (qui ne dispose d'aucune bibliothèque similaire à bokeh), vous aura peut-être convaincus des avantages et de l'accessibilité de Python pour l'analyse des données en sciences humaines et sociales. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La communauté Python s'agrandit de jours en jours et les usages de ce langage sont loin de se limiter à la science des données. \n",
    "Qui sait, peut-être tomberez-vous bientôt sur un ouvrage de python pour les sciences sociales qui vous mettra définitivement la main à la patte.  \n",
    "En attendant, merci de votre lecture :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
